{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        itmes = text.strip().split(\",\")\n",
    "        return list(map(str.strip, itmes))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(\"Hello, how, are, you\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history = load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "\n",
    "invoke_chain(\"My name is Jeon\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmessages = [\\n    SystemMessage(content=\"You are geography export.\"),\\n    AIMessage(content=\"There are many ways to measure distance\"),\\n    HumanMessage(content=\"What is the distance between Mexico and Thailand?\")\\n]\\n\\nchat.predict_messages(messages)       \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are geography export.\"),\n",
    "    AIMessage(content=\"There are many ways to measure distance\"),\n",
    "    HumanMessage(content=\"What is the distance between Mexico and Thailand?\")\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)       \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert.\"),\n",
    "    (\"ai\", \"I reply to your answers with numbers\"),\n",
    "    (\"human\", \"What is the distance between {country_a} and {country_b}.\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    country_a = \"South Korea\",\n",
    "    country_b = \"Japan\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class international chef. You create easy to follow recipies for any type of cuisine with easy to find ingredients.\"),\n",
    "    (\"human\", \"I want to cook {cuisine} food.\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\"),\n",
    "    (\"human\", \"{recipe}\"),\n",
    "])\n",
    "\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "\n",
    "final_chain = {\"recipe\" : chef_chain} | veg_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\" : \"indian\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is what I know:\n",
      "Capital: Berlin\n",
      "Language: German\n",
      "Food: Sausages, Pretzels, and Beer\n",
      "Currency: Euro. However, it's important to note that Germany was part of the Deutsche Mark zone before it adopted the Euro in 2002."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Here is what I know:\\nCapital: Berlin\\nLanguage: German\\nFood: Sausages, Pretzels, and Beer\\nCurrency: Euro. However, it's important to note that Germany was part of the Deutsche Mark zone before it adopted the Euro in 2002.\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\" : \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: France\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\" : \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\", \n",
    "    },\n",
    "    {\n",
    "        \"question\" : \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\", \n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples,\n",
    "    suffix= \"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\": \"Germany\"\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is what I know:\n",
      "Capital: Seoul\n",
      "Language: Korean (both North and South have their own versions)\n",
      "Food: Kimchi and Bulgogi\n",
      "Currency: South Korean won\n",
      "South Korea is located in East Asia and is divided into two distinct regions, North Korea and South Korea. It has a rich history with ancient ruins, dynamic cities and a unique culture all its own."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Here is what I know:\\nCapital: Seoul\\nLanguage: Korean (both North and South have their own versions)\\nFood: Kimchi and Bulgogi\\nCurrency: South Korean won\\nSouth Korea is located in East Asia and is divided into two distinct regions, North Korea and South Korea. It has a rich history with ancient ruins, dynamic cities and a unique culture all its own.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\" : \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: France\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\" : \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\", \n",
    "    },\n",
    "    {\n",
    "        \"country\" : \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\", \n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "\n",
    "chat_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert and you give short answer for easy understanding\"),\n",
    "    chat_prompt,\n",
    "    (\"human\", \"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\": \"South Korea\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about France?\\nAI:\\n        Here is what I know:\\n        Capital: Paris\\n        Language: France\\n        Food: Wine and Cheese\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "from langchain.prompts import example_selector\n",
    "\n",
    "\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\" : \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: France\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\" : \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\", \n",
    "    },\n",
    "    {\n",
    "        \"question\" : \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\", \n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arrrrgh! Me hearty loves a good plate o' pirate stew, made with fresh fish, ripe fruits, and hardtack bread. Avast ye, landlubber!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Arrrrgh! Me hearty loves a good plate o' pirate stew, made with fresh fish, ripe fruits, and hardtack bread. Avast ye, landlubber!\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are a role playing assistant.\n",
    "And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "This is an example of how you talk:\n",
    "\n",
    "Human: {example_question}\n",
    "You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Start now!\n",
    "\n",
    "Human: {question}\n",
    "You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "{intro}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(final_prompt = final, pipeline_prompts=prompts)\n",
    "\n",
    "\"\"\"\n",
    "full_prompt.format(\n",
    "    character = \"Pirate\",\n",
    "    example_question = \"What is your location?\",\n",
    "    example_answer = \"Arrrg! That is a secret!\",\n",
    "    question = \"What is your favorite food?\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"character\" : \"Pirate\",\n",
    "    \"example_question\" : \"What is your location?\",\n",
    "    \"example_answer\" : \"Arrrg! That is a secret!\",\n",
    "    \"question\" : \"What is your favorite food?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How do you make italian pasta?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOllama] [262.72s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Making authentic Italian pasta from scratch involves several steps. Here's a simplified version of the process for making classic spaghetti or fettuccine using a pasta machine:\\n\\n**Ingredients:**\\n- 200 g durum wheat semolina\\n- 3 eggs (large)\\n- Water, as needed\\n- Salt, to taste\\n\\n**Instructions:**\\n\\n1. **Prepare the dough:** Make a well in the center of a large pile of semolina on a clean, wooden surface or countertop. Crack the eggs into the well. Use a fork to gently whisk the eggs and gradually incorporate the surrounding semolina, creating a thick paste.\\n2. **Knead the dough:** Once all the semolina is incorporated, start kneading the dough using the heel of your hand, pushing it away from you and then folding it back over itself. Continue this process until the dough becomes elastic and smooth. This may take 10-15 minutes. Add a little water if necessary to keep the dough from becoming too dry.\\n3. **Rest the dough:** Wrap the dough in plastic wrap and let it rest for at least 30 minutes at room temperature. This allows the gluten to develop, making the pasta more tender and easier to roll out.\\n4. **Roll out the pasta:** Use a pasta machine to roll out the dough into thin sheets. Start by setting the pasta machine to its widest setting, and pass the dough through it several times. Gradually decrease the setting width until you reach your desired thickness. Brush the pasta sheets with semolina flour as needed to prevent sticking.\\n5. **Cut the pasta:** Once you have reached the desired sheet thickness, use a pasta cutter attachment on your machine or cut the sheets by hand into your desired pasta shape (spaghetti, fettuccine, lasagna noodles, etc.). Dust the pasta with semolina flour to prevent sticking.\\n6. **Cook the pasta:** Bring a large pot of salted water to a rolling boil. Cook the pasta for 2-5 minutes or until al dente (firm to the bite). Drain and toss with your favorite sauce. Serve immediately.\\n\\n**Tips:**\\n- Be patient and practice making the dough, as it takes time to develop the proper consistency and texture.\\n- Make sure your work surface is clean before starting, and keep the pasta sheets covered with a clean cloth to prevent them from drying out while you are working.\\n- Use high-quality ingredients for the best results.\\n- Don't overcook the pasta – it should still have a bite to it.\\n- Save some of the pasta cooking water, as it can be used to adjust the consistency of your sauce if needed.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"mistral:latest\",\n",
      "          \"created_at\": \"2024-05-30T01:09:42.7433047Z\",\n",
      "          \"response\": \"\",\n",
      "          \"done\": true,\n",
      "          \"context\": [\n",
      "            733,\n",
      "            16289,\n",
      "            28793,\n",
      "            28705,\n",
      "            733,\n",
      "            16289,\n",
      "            28793,\n",
      "            1602,\n",
      "            511,\n",
      "            368,\n",
      "            1038,\n",
      "            9126,\n",
      "            753,\n",
      "            2609,\n",
      "            28708,\n",
      "            28804,\n",
      "            733,\n",
      "            28748,\n",
      "            16289,\n",
      "            28793,\n",
      "            733,\n",
      "            28748,\n",
      "            16289,\n",
      "            28793,\n",
      "            19387,\n",
      "            18250,\n",
      "            10088,\n",
      "            2609,\n",
      "            28708,\n",
      "            477,\n",
      "            15147,\n",
      "            14657,\n",
      "            2856,\n",
      "            5944,\n",
      "            28723,\n",
      "            4003,\n",
      "            28742,\n",
      "            28713,\n",
      "            264,\n",
      "            9878,\n",
      "            1799,\n",
      "            2751,\n",
      "            302,\n",
      "            272,\n",
      "            1759,\n",
      "            354,\n",
      "            2492,\n",
      "            11495,\n",
      "            668,\n",
      "            357,\n",
      "            265,\n",
      "            21099,\n",
      "            442,\n",
      "            285,\n",
      "            2973,\n",
      "            20550,\n",
      "            473,\n",
      "            1413,\n",
      "            264,\n",
      "            2609,\n",
      "            28708,\n",
      "            5599,\n",
      "            28747,\n",
      "            13,\n",
      "            13,\n",
      "            348,\n",
      "            657,\n",
      "            28721,\n",
      "            893,\n",
      "            2785,\n",
      "            4049,\n",
      "            13,\n",
      "            28733,\n",
      "            28705,\n",
      "            28750,\n",
      "            28734,\n",
      "            28734,\n",
      "            319,\n",
      "            5598,\n",
      "            383,\n",
      "            28135,\n",
      "            3546,\n",
      "            328,\n",
      "            1380,\n",
      "            13,\n",
      "            28733,\n",
      "            28705,\n",
      "            28770,\n",
      "            14636,\n",
      "            325,\n",
      "            16962,\n",
      "            28731,\n",
      "            13,\n",
      "            28733,\n",
      "            8632,\n",
      "            28725,\n",
      "            390,\n",
      "            3236,\n",
      "            13,\n",
      "            28733,\n",
      "            21943,\n",
      "            28725,\n",
      "            298,\n",
      "            9230,\n",
      "            13,\n",
      "            13,\n",
      "            348,\n",
      "            6060,\n",
      "            8373,\n",
      "            4049,\n",
      "            13,\n",
      "            13,\n",
      "            28740,\n",
      "            28723,\n",
      "            619,\n",
      "            4277,\n",
      "            10350,\n",
      "            272,\n",
      "            24511,\n",
      "            4049,\n",
      "            6746,\n",
      "            264,\n",
      "            1162,\n",
      "            297,\n",
      "            272,\n",
      "            4982,\n",
      "            302,\n",
      "            264,\n",
      "            2475,\n",
      "            17972,\n",
      "            302,\n",
      "            3546,\n",
      "            328,\n",
      "            1380,\n",
      "            356,\n",
      "            264,\n",
      "            3587,\n",
      "            28725,\n",
      "            12602,\n",
      "            5439,\n",
      "            442,\n",
      "            5573,\n",
      "            3746,\n",
      "            28723,\n",
      "            4603,\n",
      "            468,\n",
      "            272,\n",
      "            14636,\n",
      "            778,\n",
      "            272,\n",
      "            1162,\n",
      "            28723,\n",
      "            5938,\n",
      "            264,\n",
      "            24143,\n",
      "            298,\n",
      "            14259,\n",
      "            26898,\n",
      "            272,\n",
      "            14636,\n",
      "            304,\n",
      "            17885,\n",
      "            26505,\n",
      "            272,\n",
      "            12028,\n",
      "            3546,\n",
      "            328,\n",
      "            1380,\n",
      "            28725,\n",
      "            6818,\n",
      "            264,\n",
      "            6931,\n",
      "            26043,\n",
      "            28723,\n",
      "            13,\n",
      "            28750,\n",
      "            28723,\n",
      "            619,\n",
      "            28796,\n",
      "            485,\n",
      "            316,\n",
      "            272,\n",
      "            24511,\n",
      "            4049,\n",
      "            5713,\n",
      "            544,\n",
      "            272,\n",
      "            3546,\n",
      "            328,\n",
      "            1380,\n",
      "            349,\n",
      "            23799,\n",
      "            28725,\n",
      "            1149,\n",
      "            7770,\n",
      "            7836,\n",
      "            272,\n",
      "            24511,\n",
      "            1413,\n",
      "            272,\n",
      "            400,\n",
      "            301,\n",
      "            302,\n",
      "            574,\n",
      "            1021,\n",
      "            28725,\n",
      "            13884,\n",
      "            378,\n",
      "            1753,\n",
      "            477,\n",
      "            368,\n",
      "            304,\n",
      "            868,\n",
      "            10068,\n",
      "            288,\n",
      "            378,\n",
      "            852,\n",
      "            754,\n",
      "            3837,\n",
      "            28723,\n",
      "            13718,\n",
      "            441,\n",
      "            456,\n",
      "            1759,\n",
      "            1996,\n",
      "            272,\n",
      "            24511,\n",
      "            6755,\n",
      "            639,\n",
      "            3953,\n",
      "            304,\n",
      "            7898,\n",
      "            28723,\n",
      "            851,\n",
      "            993,\n",
      "            1388,\n",
      "            28705,\n",
      "            28740,\n",
      "            28734,\n",
      "            28733,\n",
      "            28740,\n",
      "            28782,\n",
      "            3486,\n",
      "            28723,\n",
      "            3301,\n",
      "            264,\n",
      "            1628,\n",
      "            2130,\n",
      "            513,\n",
      "            4892,\n",
      "            298,\n",
      "            1840,\n",
      "            272,\n",
      "            24511,\n",
      "            477,\n",
      "            7888,\n",
      "            1368,\n",
      "            6964,\n",
      "            28723,\n",
      "            13,\n",
      "            28770,\n",
      "            28723,\n",
      "            619,\n",
      "            9133,\n",
      "            272,\n",
      "            24511,\n",
      "            4049,\n",
      "            394,\n",
      "            1242,\n",
      "            272,\n",
      "            24511,\n",
      "            297,\n",
      "            10409,\n",
      "            12551,\n",
      "            304,\n",
      "            1346,\n",
      "            378,\n",
      "            1846,\n",
      "            354,\n",
      "            438,\n",
      "            2429,\n",
      "            28705,\n",
      "            28770,\n",
      "            28734,\n",
      "            3486,\n",
      "            438,\n",
      "            2003,\n",
      "            7641,\n",
      "            28723,\n",
      "            851,\n",
      "            5976,\n",
      "            272,\n",
      "            1272,\n",
      "            8142,\n",
      "            298,\n",
      "            1950,\n",
      "            28725,\n",
      "            2492,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            680,\n",
      "            20602,\n",
      "            304,\n",
      "            7089,\n",
      "            298,\n",
      "            4595,\n",
      "            575,\n",
      "            28723,\n",
      "            13,\n",
      "            28781,\n",
      "            28723,\n",
      "            619,\n",
      "            22849,\n",
      "            575,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            4049,\n",
      "            5938,\n",
      "            264,\n",
      "            2609,\n",
      "            28708,\n",
      "            5599,\n",
      "            298,\n",
      "            4595,\n",
      "            575,\n",
      "            272,\n",
      "            24511,\n",
      "            778,\n",
      "            9026,\n",
      "            18763,\n",
      "            28723,\n",
      "            7043,\n",
      "            486,\n",
      "            5587,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            5599,\n",
      "            298,\n",
      "            871,\n",
      "            7744,\n",
      "            374,\n",
      "            5587,\n",
      "            28725,\n",
      "            304,\n",
      "            1455,\n",
      "            272,\n",
      "            24511,\n",
      "            1059,\n",
      "            378,\n",
      "            2856,\n",
      "            2421,\n",
      "            28723,\n",
      "            17252,\n",
      "            1323,\n",
      "            17889,\n",
      "            272,\n",
      "            5587,\n",
      "            4850,\n",
      "            1996,\n",
      "            368,\n",
      "            4563,\n",
      "            574,\n",
      "            11785,\n",
      "            28032,\n",
      "            28723,\n",
      "            1896,\n",
      "            1426,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            18763,\n",
      "            395,\n",
      "            3546,\n",
      "            328,\n",
      "            1380,\n",
      "            14826,\n",
      "            390,\n",
      "            3236,\n",
      "            298,\n",
      "            5297,\n",
      "            27322,\n",
      "            28723,\n",
      "            13,\n",
      "            28782,\n",
      "            28723,\n",
      "            619,\n",
      "            27724,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            4049,\n",
      "            5713,\n",
      "            368,\n",
      "            506,\n",
      "            5048,\n",
      "            272,\n",
      "            11785,\n",
      "            12173,\n",
      "            28032,\n",
      "            28725,\n",
      "            938,\n",
      "            264,\n",
      "            2609,\n",
      "            28708,\n",
      "            3119,\n",
      "            360,\n",
      "            22417,\n",
      "            356,\n",
      "            574,\n",
      "            5599,\n",
      "            442,\n",
      "            3119,\n",
      "            272,\n",
      "            18763,\n",
      "            486,\n",
      "            1021,\n",
      "            778,\n",
      "            574,\n",
      "            11785,\n",
      "            2609,\n",
      "            28708,\n",
      "            5843,\n",
      "            325,\n",
      "            886,\n",
      "            357,\n",
      "            265,\n",
      "            21099,\n",
      "            28725,\n",
      "            285,\n",
      "            2973,\n",
      "            20550,\n",
      "            473,\n",
      "            28725,\n",
      "            2635,\n",
      "            22408,\n",
      "            708,\n",
      "            350,\n",
      "            867,\n",
      "            28725,\n",
      "            4345,\n",
      "            15745,\n",
      "            384,\n",
      "            469,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            395,\n",
      "            3546,\n",
      "            328,\n",
      "            1380,\n",
      "            14826,\n",
      "            298,\n",
      "            5297,\n",
      "            27322,\n",
      "            28723,\n",
      "            13,\n",
      "            28784,\n",
      "            28723,\n",
      "            619,\n",
      "            28743,\n",
      "            600,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            4049,\n",
      "            24786,\n",
      "            264,\n",
      "            2475,\n",
      "            2513,\n",
      "            302,\n",
      "            9685,\n",
      "            286,\n",
      "            2130,\n",
      "            298,\n",
      "            264,\n",
      "            15483,\n",
      "            1359,\n",
      "            309,\n",
      "            28723,\n",
      "            13092,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            354,\n",
      "            28705,\n",
      "            28750,\n",
      "            28733,\n",
      "            28782,\n",
      "            3486,\n",
      "            442,\n",
      "            1996,\n",
      "            389,\n",
      "            281,\n",
      "            3639,\n",
      "            325,\n",
      "            28722,\n",
      "            2780,\n",
      "            298,\n",
      "            272,\n",
      "            18082,\n",
      "            609,\n",
      "            2985,\n",
      "            426,\n",
      "            304,\n",
      "            13647,\n",
      "            395,\n",
      "            574,\n",
      "            6656,\n",
      "            16042,\n",
      "            28723,\n",
      "            3066,\n",
      "            333,\n",
      "            5347,\n",
      "            28723,\n",
      "            13,\n",
      "            13,\n",
      "            348,\n",
      "            28738,\n",
      "            2430,\n",
      "            4049,\n",
      "            13,\n",
      "            28733,\n",
      "            1739,\n",
      "            7749,\n",
      "            304,\n",
      "            5245,\n",
      "            2492,\n",
      "            272,\n",
      "            24511,\n",
      "            28725,\n",
      "            390,\n",
      "            378,\n",
      "            4347,\n",
      "            727,\n",
      "            298,\n",
      "            1950,\n",
      "            272,\n",
      "            4979,\n",
      "            23074,\n",
      "            304,\n",
      "            14024,\n",
      "            28723,\n",
      "            13,\n",
      "            28733,\n",
      "            6746,\n",
      "            1864,\n",
      "            574,\n",
      "            771,\n",
      "            5439,\n",
      "            349,\n",
      "            3587,\n",
      "            1159,\n",
      "            5615,\n",
      "            28725,\n",
      "            304,\n",
      "            1840,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            18763,\n",
      "            6823,\n",
      "            395,\n",
      "            264,\n",
      "            3587,\n",
      "            17673,\n",
      "            298,\n",
      "            5297,\n",
      "            706,\n",
      "            477,\n",
      "            281,\n",
      "            5125,\n",
      "            575,\n",
      "            1312,\n",
      "            368,\n",
      "            460,\n",
      "            2739,\n",
      "            28723,\n",
      "            13,\n",
      "            28733,\n",
      "            5938,\n",
      "            1486,\n",
      "            28733,\n",
      "            14817,\n",
      "            13506,\n",
      "            354,\n",
      "            272,\n",
      "            1489,\n",
      "            2903,\n",
      "            28723,\n",
      "            13,\n",
      "            28733,\n",
      "            3189,\n",
      "            28742,\n",
      "            28707,\n",
      "            754,\n",
      "            27177,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            764,\n",
      "            378,\n",
      "            1023,\n",
      "            1309,\n",
      "            506,\n",
      "            264,\n",
      "            18082,\n",
      "            298,\n",
      "            378,\n",
      "            28723,\n",
      "            13,\n",
      "            28733,\n",
      "            15226,\n",
      "            741,\n",
      "            302,\n",
      "            272,\n",
      "            2609,\n",
      "            28708,\n",
      "            13198,\n",
      "            2130,\n",
      "            28725,\n",
      "            390,\n",
      "            378,\n",
      "            541,\n",
      "            347,\n",
      "            1307,\n",
      "            298,\n",
      "            7392,\n",
      "            272,\n",
      "            23074,\n",
      "            302,\n",
      "            574,\n",
      "            16042,\n",
      "            513,\n",
      "            3236,\n",
      "            28723\n",
      "          ],\n",
      "          \"total_duration\": 260637975900,\n",
      "          \"load_duration\": 2225829400,\n",
      "          \"prompt_eval_count\": 25,\n",
      "          \"prompt_eval_duration\": 4693467000,\n",
      "          \"eval_count\": 615,\n",
      "          \"eval_duration\": 253714876000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" Making authentic Italian pasta from scratch involves several steps. Here's a simplified version of the process for making classic spaghetti or fettuccine using a pasta machine:\\n\\n**Ingredients:**\\n- 200 g durum wheat semolina\\n- 3 eggs (large)\\n- Water, as needed\\n- Salt, to taste\\n\\n**Instructions:**\\n\\n1. **Prepare the dough:** Make a well in the center of a large pile of semolina on a clean, wooden surface or countertop. Crack the eggs into the well. Use a fork to gently whisk the eggs and gradually incorporate the surrounding semolina, creating a thick paste.\\n2. **Knead the dough:** Once all the semolina is incorporated, start kneading the dough using the heel of your hand, pushing it away from you and then folding it back over itself. Continue this process until the dough becomes elastic and smooth. This may take 10-15 minutes. Add a little water if necessary to keep the dough from becoming too dry.\\n3. **Rest the dough:** Wrap the dough in plastic wrap and let it rest for at least 30 minutes at room temperature. This allows the gluten to develop, making the pasta more tender and easier to roll out.\\n4. **Roll out the pasta:** Use a pasta machine to roll out the dough into thin sheets. Start by setting the pasta machine to its widest setting, and pass the dough through it several times. Gradually decrease the setting width until you reach your desired thickness. Brush the pasta sheets with semolina flour as needed to prevent sticking.\\n5. **Cut the pasta:** Once you have reached the desired sheet thickness, use a pasta cutter attachment on your machine or cut the sheets by hand into your desired pasta shape (spaghetti, fettuccine, lasagna noodles, etc.). Dust the pasta with semolina flour to prevent sticking.\\n6. **Cook the pasta:** Bring a large pot of salted water to a rolling boil. Cook the pasta for 2-5 minutes or until al dente (firm to the bite). Drain and toss with your favorite sauce. Serve immediately.\\n\\n**Tips:**\\n- Be patient and practice making the dough, as it takes time to develop the proper consistency and texture.\\n- Make sure your work surface is clean before starting, and keep the pasta sheets covered with a clean cloth to prevent them from drying out while you are working.\\n- Use high-quality ingredients for the best results.\\n- Don't overcook the pasta – it should still have a bite to it.\\n- Save some of the pasta cooking water, as it can be used to adjust the consistency of your sauce if needed.\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Making authentic Italian pasta from scratch involves several steps. Here's a simplified version of the process for making classic spaghetti or fettuccine using a pasta machine:\\n\\n**Ingredients:**\\n- 200 g durum wheat semolina\\n- 3 eggs (large)\\n- Water, as needed\\n- Salt, to taste\\n\\n**Instructions:**\\n\\n1. **Prepare the dough:** Make a well in the center of a large pile of semolina on a clean, wooden surface or countertop. Crack the eggs into the well. Use a fork to gently whisk the eggs and gradually incorporate the surrounding semolina, creating a thick paste.\\n2. **Knead the dough:** Once all the semolina is incorporated, start kneading the dough using the heel of your hand, pushing it away from you and then folding it back over itself. Continue this process until the dough becomes elastic and smooth. This may take 10-15 minutes. Add a little water if necessary to keep the dough from becoming too dry.\\n3. **Rest the dough:** Wrap the dough in plastic wrap and let it rest for at least 30 minutes at room temperature. This allows the gluten to develop, making the pasta more tender and easier to roll out.\\n4. **Roll out the pasta:** Use a pasta machine to roll out the dough into thin sheets. Start by setting the pasta machine to its widest setting, and pass the dough through it several times. Gradually decrease the setting width until you reach your desired thickness. Brush the pasta sheets with semolina flour as needed to prevent sticking.\\n5. **Cut the pasta:** Once you have reached the desired sheet thickness, use a pasta cutter attachment on your machine or cut the sheets by hand into your desired pasta shape (spaghetti, fettuccine, lasagna noodles, etc.). Dust the pasta with semolina flour to prevent sticking.\\n6. **Cook the pasta:** Bring a large pot of salted water to a rolling boil. Cook the pasta for 2-5 minutes or until al dente (firm to the bite). Drain and toss with your favorite sauce. Serve immediately.\\n\\n**Tips:**\\n- Be patient and practice making the dough, as it takes time to develop the proper consistency and texture.\\n- Make sure your work surface is clean before starting, and keep the pasta sheets covered with a clean cloth to prevent them from drying out while you are working.\\n- Use high-quality ingredients for the best results.\\n- Don't overcook the pasta – it should still have a bite to it.\\n- Save some of the pasta cooking water, as it can be used to adjust the consistency of your sauce if needed.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "set_debug(True)\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "chat.predict(\"How do you make italian pasta?\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k = 4\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(1, 1)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "add_message(5, 5)\n",
    "memory.load_memory_variables({})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=chat)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Jeon, I live in Tokyo\", \"Wow that is so cool!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "\n",
    "add_message(\"South Kddorea is so pretty\", \"I wish I could go!!!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "\n",
    "add_message(\"How far is Brazil from Argentina?\", \"I don't know! Super far!\")\n",
    "\n",
    "add_message(\"How far is Brazil from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "\n",
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})\n",
    "\n",
    "\n",
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\n",
    "\n",
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m\n\u001b[0;32m      9\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationSummaryBufferMemory(\n\u001b[0;32m     10\u001b[0m     llm\u001b[38;5;241m=\u001b[39mchat,\n\u001b[0;32m     11\u001b[0m     max_token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m,\n\u001b[0;32m     12\u001b[0m     memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124m    You are a helpful AI talking to a human.\u001b[39m\n\u001b[0;32m     17\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124m    You:\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     23\u001b[0m chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m---> 24\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[43mllm\u001b[49m,\n\u001b[0;32m     25\u001b[0m     memory\u001b[38;5;241m=\u001b[39mmemory,\n\u001b[0;32m     26\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template),\n\u001b[0;32m     27\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m chain\u001b[38;5;241m.\u001b[39mpredict(question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy name is Nico\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question = \"My name is Nico\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question = \"I live in tokyo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question = \"What's my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
